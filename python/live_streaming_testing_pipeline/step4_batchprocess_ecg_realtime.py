import numpy as np
from scipy.signal import butter, sosfilt, sosfilt_zi

from datetime import datetime

# ========= IMPORT CLASSES FROM CORE and STEP1 (GENERATE DATASET) ===================================================================

from python.pipeline.step1_generate_dataset_physionet import generateData
from python.pipeline.step1_generate_dataset_physionet import (
    main as generate_dataset_main,
)
from python.core.signal_processing import (
    R_peak_detector,
    BPMDetector,
    AD8232_Bandpass_Simulator,
)
from python.core.logging import CSVLogger

# ====================================================================================================================================


import csv
import os


# ----------------------------
# CSV Data Loader
# ----------------------------


def load_dataset_from_csv(csv_path):
    """
    Load digital dataset from streamed_raw_packets.csv generated by step3_stream_ble_realtime.py

    Args:
        csv_path: Path to streamed_raw_packets.csv file

    Returns:
        List of digital ADC values (Sample column)
    """
    import pandas as pd

    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV file not found: {csv_path}")

    # Read CSV file
    df = pd.read_csv(csv_path)

    # Extract Sample column and convert to list
    digital_dataset = df["Sample"].tolist()

    print(f"✅ Loaded {len(digital_dataset)} samples from {csv_path}")

    return digital_dataset


# ----------------------------
# BatchTester Wrapper
# ----------------------------


class BatchTester:
    """
    Runs the exact same logic as streaming mode, but on pre-recorded data.
    """

    def __init__(self, fs=250, use_filter=False):
        self.fs = fs
        self.use_filter = use_filter  # choose whether or not to apply bandpass filter
        self.detector = R_peak_detector(fs=fs)
        self.bpm_detector = BPMDetector(fs=fs)
        self.filter = AD8232_Bandpass_Simulator(fs=fs) if use_filter else None

    def run(self, data):
        if self.use_filter:
            data = self.filter.filter_array(data)

        timestamps = np.arange(len(data)) / self.fs

        for i, sample in enumerate(data):
            # Track peaks before processing
            peaks_before = len(self.detector.detected_peaks)

            self.detector.process_sample(sample)

            # Check if a new peak was just detected
            peaks_after = len(self.detector.detected_peaks)
            is_peak = peaks_after > peaks_before  # ✅ Correct check

            if is_peak:
                # Use the detected peak's index, not the current loop index
                peak_sample_index = self.detector.detected_peaks[-1]
                peak_timestamp = (
                    peak_sample_index / self.fs
                )  # Calculate timestamp from peak index

                self.bpm_detector.add_peak(peak_sample_index, peak_timestamp)
        # Calculate windowed BPM at the end using the last timestamp
        if len(timestamps) > 0:
            final_timestamp = timestamps[-1]
            final_windowed_bpm = self.bpm_detector.calculate_bpm_in_window(
                final_timestamp
            )

        return {
            "r_peaks": self.detector.detected_peaks,
            "instantaneous_bpm": self.bpm_detector.bpm_history,
            "avg_bpm_5s": self.bpm_detector.current_bpm,
        }


def main(csv_file_path=None, output_csv_path=None):
    """
    Run batch processing on ECG data from streamed CSV file.

    Args:
        csv_file_path: Path to streamed_raw_packets.csv from step3
        output_csv_path: Directory to save batch processing results


    Returns:
        Path to saved batch processing CSV file
    """

    # Load digital dataset from CSV file (streamed data from step3)
    if csv_file_path:
        print(f"Loading streamed dataset from {csv_file_path}...")
        digital_dataset = load_dataset_from_csv(csv_file_path)
    else:
        raise ValueError(
            "csv_file_path is required - must provide path to streamed_raw_packets.csv"
        )

    # Check that dataset is not empty
    if not digital_dataset or len(digital_dataset) == 0:
        raise ValueError("Digital dataset is empty - no samples loaded from CSV")

    # Run batch processing
    print(f"Running batch processing on {len(digital_dataset)} samples...")
    tester = BatchTester(fs=250, use_filter=False)
    results = tester.run(digital_dataset)

    detected_peaks = results["r_peaks"]
    bpm_history = results["instantaneous_bpm"]
    avg_bpm_5s = results["avg_bpm_5s"]

    print(f"Detected peaks: {detected_peaks}")
    print(f"BPM history (instantaneous): {bpm_history}")
    print(f"Averaged BPM (per every 5 seconds): {round(float(avg_bpm_5s),1)}")

    csv_filename = os.path.join(output_csv_path, "batch_processed_outputs.csv")

    min_len = min(len(detected_peaks) - 1, len(bpm_history))
    with open(csv_filename, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Detected R_peak_index", "Digital Value", "Instantaneous_BPM"])
        for i in range(min_len):
            r_index = detected_peaks[i]
            r_value_dig = digital_dataset[r_index]
            bpm = bpm_history[i]
            writer.writerow([r_index, r_value_dig, bpm])
    print(f"✅ Saved detected R-peak and BPM data (batch-processing) to {csv_filename}")
    return csv_filename


if __name__ == "__main__":
    main()
